{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-mozambique",
   "metadata": {},
   "source": [
    "# ğŸ¼ ì¸ê³µì§€ëŠ¥ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°\n",
    "\n",
    "#### \"ë‹¨ì–´ë¥¼ ë„£ìœ¼ë©´ ê°€ì‚¬ë¥¼(í•œ ì¤„) ì¨ì£¼ëŠ” í”„ë¡œê·¸ë¨ ì œì‘\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-queue",
   "metadata": {},
   "source": [
    "### ğŸ˜® ëª©ì°¨\n",
    "\n",
    "**1. ë°ì´í„° ì¤€ë¹„**   \n",
    "**2. ë°ì´í„° ì „ì²˜ë¦¬**     \n",
    "**3. ëª¨ë¸ ì„¤ê³„, í›ˆë ¨, í‰ê°€**   \n",
    "**4. ê°€ì‚¬ ìƒì„±í•˜ê¸°**   \n",
    "**5. ê³ ì°°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-particular",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-privacy",
   "metadata": {},
   "source": [
    "**ëª¨ë“ˆ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naval-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-improvement",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-present",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatty-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\", 'I grew strong', \"I learned how to get along And so you're back\", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock', 'I would have made you leave your key', 'If I had known for just one second', \"You'd be back to bother me Well now go,\", 'Walk out the door', 'Just turn around', \"Now, you're not welcome anymore Weren't you the one\", 'Who tried to break me with desire?', \"Did you think I'd crumble?\", \"Did you think I'd lay down and die? Oh not I,\", 'I will survive', 'Yeah', \"As Long as I know how to love, I know I'll be alive\", \"I've got all my life to live\", \"I've got all my love to give\", 'I will survive, I will survive', 'Yeah, yeah', 'It took all the strength I had', \"Just not to fall apart I'm trying hard to mend the pieces\", 'Of my broken heart', 'And I spent oh so many nights']\n"
     ]
    }
   ],
   "source": [
    "text_file_path = os.getenv('HOME')+'/aiffel/aiffel_exploration/lyricist/data/lyrics/*'\n",
    "text_list = glob.glob(text_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "for text_file in text_list:\n",
    "    with open(text_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-policy",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-natural",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-catholic",
   "metadata": {},
   "source": [
    "### 2.1 ë¬¸ì¥ ì •ì œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparable-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ì œ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , ì–‘ìª½ ê³µë°± ì§€ìš°ê¸°\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence) # íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°± ë„£ê¸°\n",
    "    sentence = re.sub(r\"[' ']+\", ' ', sentence) # ì—¬ëŸ¬ ê³µë°±ì€ í•˜ë‚˜ë¡œ ë°”ê¾¸ê¸°\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", ' ', sentence) # í•´ë‹¹ë¬¸ì ì´ì™¸ì˜ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
    "    sentence = sentence.strip() # ì–‘ìª½ ê³µë°± ì§€ìš°ê¸°\n",
    "    sentence = '<start> ' + sentence + ' <end>' # ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì— ë¼ë²¨ ë¶™ì´ê¸°\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decimal-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> i would have made you leave your key <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì •ì œëœ ë¬¸ì¥ ëª¨ìœ¼ê¸°\n",
    "corpus = []\n",
    "for sentence in raw_corpus: # í•œ ë¬¸ì¥ì”© êº¼ë‚´ê¸°\n",
    "    if len(sentence) == 0 or len(sentence) == 1: # ë¬¸ì¥ì˜ ê¸¸ì´ê°€ 0, 1ì´ë©´ ì €ì¥ì•ˆí•¨\n",
    "        continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence) # ë¬¸ì¥ ì •ì œí•˜ê¸°\n",
    "    if len(preprocessed_sentence.split()) > 15: # ì •ì œëœ ë¬¸ì¥ì´ 15ë‹¨ì–´ ì´ìƒì´ë©´ ì €ì¥ì•ˆí•¨\n",
    "        continue     \n",
    "    corpus.append(preprocessed_sentence) # ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-karaoke",
   "metadata": {},
   "source": [
    "### 2.2 ë¬¸ì¥ í† í°í™”í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boring-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í°í™” í•¨ìˆ˜\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ', oov_token=\"<unk>\") # 12000ë‹¨ì–´ë¥¼ ê¸°ì–µ, ì´ë¯¸ ë¬¸ì¥ ì •ì œí•´ì„œ filter í•„ìš” ì—†ìŒ, 12000ë‹¨ì–´ ì´ì™¸ì˜ ë‹¨ì–´ëŠ” \"<unk>\"ë¡œ ì·¨ê¸‰\n",
    "    tokenizer.fit_on_texts(corpus) # ë‹¨ì–´ì‚¬ì „ ìƒì„±\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # ë°ì´í„°ë¥¼ ë²¡í„°í™”\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # ì…ë ¥ë°ì´í„° ì‹œí€¸ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶¤. ì‹œí€€ìŠ¤ê°€ ì§§ìœ¼ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”© ë¶™ìŒ. padding='pre'ëŠ” ì•ì— ë¶™ìŒ\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus) # ë¬¸ì¥, ë‹¨ì–´ì‚¬ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distinguished-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156192, 15)\n",
      "[[   2   70  247    4   53  708    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    4   53 6269    3    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    4 1066  525    4  104   80  192  257    7    3    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° í™•ì¸\n",
    "print(tensor.shape) # 15ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ 156192ê°œì˜ ë¬¸ì¥\n",
    "print(tensor[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beneficial-tradition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ì‚¬ì „ í™•ì¸\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-puppy",
   "metadata": {},
   "source": [
    "### 2.3 ë°ì´í„°ì…‹ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "through-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  70 247   4  53 708   3   0   0   0   0   0   0   0]\n",
      "[ 70 247   4  53 708   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# ì†ŒìŠ¤ë¬¸ì¥, íƒ€ê²Ÿë¬¸ì¥ ìƒì„±í•˜ê¸°\n",
    "src_input = tensor[:, :-1] # ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ\n",
    "tgt_input = tensor[:, 1:] # tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ ìƒì„±\n",
    "\n",
    "print(src_input[0]) # ë§¨ ë’¤ ì¸ë±ìŠ¤ ì œê±° ìƒíƒœ\n",
    "print(tgt_input[0]) # ë§¨ ì• ì¸ë±ìŠ¤ ì œê±° ìƒíƒœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifty-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124953, 14)\n",
      "Target Train: (124953, 14)\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ë°ì´í„°, í‰ê°€ë°ì´í„° ë¶„ë¦¬\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=9)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "federal-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-cisco",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-center",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ì„¤ê³„, í›ˆë ¨, í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-demographic",
   "metadata": {},
   "source": [
    "### 3.1 ëª¨ë¸ ì„¤ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beginning-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # ë‹¨ì–´ì‚¬ì „ì˜ ì¸ë±ìŠ¤ë¥¼ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì›Œë“œ ë²¡í„°ë¡œ ë°”ê¿ˆ \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True) \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "vocab_size = tokenizer.num_words + 1 # 12000 + pad  \n",
    "embedding_size = 256\n",
    "hidden_size = 1500\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fallen-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 3.49633367e-04, -4.79755865e-04,  2.33902945e-04, ...,\n",
       "         -1.96660967e-05, -4.43540339e-04,  4.02360543e-04],\n",
       "        [ 3.23095592e-04, -5.30259917e-04,  1.94919368e-04, ...,\n",
       "         -2.16977660e-05, -4.78130125e-04,  1.00855366e-03],\n",
       "        ...,\n",
       "        [ 2.19061249e-03, -7.25672347e-04,  4.15295595e-04, ...,\n",
       "          1.65148627e-03, -4.48797422e-04,  2.24689837e-03],\n",
       "        [ 2.58490234e-03, -2.63625989e-04,  4.60571813e-04, ...,\n",
       "          1.91962195e-03, -2.33269966e-04,  2.28238688e-03],\n",
       "        [ 2.93728127e-03,  2.19953203e-04,  4.89130209e-04, ...,\n",
       "          2.11978983e-03, -1.57870127e-05,  2.32613203e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 5.19168061e-05, -2.92468263e-04,  6.83102626e-05, ...,\n",
       "          9.85752704e-05, -4.62379598e-04, -5.52645943e-05],\n",
       "        [ 1.52741966e-04, -4.44839912e-04,  1.01745027e-04, ...,\n",
       "         -4.09482818e-05, -5.13952400e-04, -1.26020997e-04],\n",
       "        ...,\n",
       "        [ 1.83100789e-03,  4.78931353e-04,  4.56420297e-04, ...,\n",
       "          2.06716172e-03,  4.67486505e-04,  1.20177958e-03],\n",
       "        [ 2.27331859e-03,  9.02572239e-04,  5.20350120e-04, ...,\n",
       "          2.24718312e-03,  6.39829144e-04,  1.44978438e-03],\n",
       "        [ 2.67182570e-03,  1.32721767e-03,  5.70759934e-04, ...,\n",
       "          2.37469561e-03,  7.97321089e-04,  1.67556701e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [-4.18869386e-05, -3.84855201e-04, -1.78509028e-04, ...,\n",
       "          1.17171490e-04, -2.49970413e-04,  3.71594360e-04],\n",
       "        [-2.87309173e-04, -3.12132121e-04, -3.08558054e-04, ...,\n",
       "         -9.86980085e-05, -2.32570455e-04,  6.26410241e-04],\n",
       "        ...,\n",
       "        [ 1.04327302e-03,  4.50637337e-04, -2.23918294e-04, ...,\n",
       "          1.04025577e-03,  7.13730638e-04,  1.30405044e-03],\n",
       "        [ 1.49795495e-03,  6.30922092e-04, -1.52855253e-04, ...,\n",
       "          1.39481912e-03,  8.38164182e-04,  1.47304509e-03],\n",
       "        [ 1.92514271e-03,  8.77473154e-04, -7.61426563e-05, ...,\n",
       "          1.69858744e-03,  9.50766902e-04,  1.64174242e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [-1.08631139e-04, -1.26508749e-04, -2.57421983e-04, ...,\n",
       "         -4.66796337e-05, -4.96760709e-04, -4.26693296e-05],\n",
       "        [-1.36202361e-04, -1.35887152e-04, -3.32299416e-04, ...,\n",
       "         -3.75149975e-05, -5.71071054e-04, -2.19226044e-04],\n",
       "        ...,\n",
       "        [ 2.09059892e-03,  5.62418893e-04,  4.11125366e-04, ...,\n",
       "          2.13874341e-03,  3.77763790e-04,  6.70402602e-04],\n",
       "        [ 2.46163574e-03,  8.89785704e-04,  4.53985442e-04, ...,\n",
       "          2.34731100e-03,  6.14915625e-04,  9.57512937e-04],\n",
       "        [ 2.80239363e-03,  1.24323950e-03,  4.90898616e-04, ...,\n",
       "          2.49508861e-03,  8.32190912e-04,  1.22424553e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 1.80454481e-05, -4.57068672e-04,  3.52210336e-05, ...,\n",
       "          6.03899862e-05, -3.34966142e-04, -9.82261481e-05],\n",
       "        [-1.86154226e-04, -7.27543142e-04, -7.16369395e-05, ...,\n",
       "          2.91030126e-04, -2.31985061e-04, -1.38864663e-04],\n",
       "        ...,\n",
       "        [ 2.23370595e-03,  5.23134950e-04,  5.78967680e-04, ...,\n",
       "          1.75671827e-03,  7.22748809e-04,  1.58406631e-03],\n",
       "        [ 2.65683001e-03,  9.53179668e-04,  6.34717464e-04, ...,\n",
       "          1.95636274e-03,  8.40177294e-04,  1.78415619e-03],\n",
       "        [ 3.02647869e-03,  1.36979774e-03,  6.65608502e-04, ...,\n",
       "          2.10457179e-03,  9.52058996e-04,  1.96472183e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 2.41917965e-04, -5.17658307e-04, -4.72484026e-05, ...,\n",
       "          2.58169166e-04, -1.17056516e-04,  5.91853423e-05],\n",
       "        [ 2.06966404e-04, -6.45298511e-04,  1.02968763e-04, ...,\n",
       "          3.06783826e-04, -9.98731630e-05,  2.58485903e-04],\n",
       "        ...,\n",
       "        [ 2.17729318e-03,  7.43298617e-04,  3.02359113e-04, ...,\n",
       "          2.00338592e-03,  5.24481933e-04,  2.20783567e-03],\n",
       "        [ 2.56654597e-03,  1.15957833e-03,  3.29909293e-04, ...,\n",
       "          2.18390441e-03,  6.58618344e-04,  2.31957505e-03],\n",
       "        [ 2.91374419e-03,  1.56851392e-03,  3.45860433e-04, ...,\n",
       "          2.31188349e-03,  7.86948600e-04,  2.41753273e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë³¸ê²©ì ì¸ í•™ìŠµ ì „ ëª¨ë¸ì— ë°ì´í„° ì¼ë¶€ íƒœì›Œë³´ê¸°\n",
    "for src_sample, tgt_sample in dataset.take(1): # take(1) 1ê°œì˜ ë°°ì¹˜. 256ê°œì˜ ë¬¸ì¥ ë°ì´í„° ê°€ì ¸ì˜´\n",
    "    break\n",
    "    \n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecological-oxygen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  10542000  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  18006000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  18013501  \n",
      "=================================================================\n",
      "Total params: 49,633,757\n",
      "Trainable params: 49,633,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # ì¶œë ¥ í˜•íƒœê°€ ë¹„ì •í˜•. ëª¨ë¸ì´ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì„ì˜ì˜ ê°’ì„ ë„£ì–´ì£¼ë©´ ì„¤ì •ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-density",
   "metadata": {},
   "source": [
    "### 3.2 ëª¨ë¸ í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vital-ferry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "488/488 [==============================] - 271s 550ms/step - loss: 4.0288\n",
      "Epoch 2/10\n",
      "488/488 [==============================] - 271s 554ms/step - loss: 3.0689\n",
      "Epoch 3/10\n",
      "488/488 [==============================] - 272s 558ms/step - loss: 2.8699\n",
      "Epoch 4/10\n",
      "488/488 [==============================] - 273s 559ms/step - loss: 2.7071\n",
      "Epoch 5/10\n",
      "488/488 [==============================] - 273s 560ms/step - loss: 2.5744\n",
      "Epoch 6/10\n",
      "488/488 [==============================] - 274s 560ms/step - loss: 2.4598\n",
      "Epoch 7/10\n",
      "488/488 [==============================] - 272s 558ms/step - loss: 2.3521\n",
      "Epoch 8/10\n",
      "488/488 [==============================] - 273s 558ms/step - loss: 2.2422\n",
      "Epoch 9/10\n",
      "488/488 [==============================] - 273s 558ms/step - loss: 2.1465\n",
      "Epoch 10/10\n",
      "488/488 [==============================] - 272s 558ms/step - loss: 2.0535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ed03ae910>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-constraint",
   "metadata": {},
   "source": [
    "### 3.3 ëª¨ë¸  í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conditional-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977/977 [==============================] - 47s 47ms/step - loss: 2.4457\n",
      "val_loss: 2.4456546306610107\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val, dec_val)\n",
    "print(\"val_loss:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-majority",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-mechanism",
   "metadata": {},
   "source": [
    "## 4. ê°€ì‚¬ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "juvenile-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ìƒì„±ê¸°\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=17):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True: # í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í•´ì•¼ í•˜ëŠ” ì‹œì ì—ì„œ ì†ŒìŠ¤ë¬¸ì¥, íƒ€ê²Ÿë¬¸ì¥ì´ ì—†ë‹¤.\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # ì˜ˆì¸¡ ë‹¨ì–´ ìƒì„±\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # ìƒì„± ë‹¨ì–´ ì´ì–´ë¶™ì´ê¸°\n",
    "        if predict_word.numpy()[0] == end_token: # ë§ˆì§€ë§‰ì´ <end>ë©´ ì¢…ë£Œ\n",
    "            break\n",
    "        if test_tensor.shape[1] > max_len: # <start>, <end> ì œì™¸í•œ ë‹¨ì–´ ê°¯ìˆ˜ê°€ 15ë¥¼ ë„˜ê¸°ë©´ ì¢…ë£Œ \n",
    "            break\n",
    "            \n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy(): # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "designed-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the one <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì¥ ìƒì„±1\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "binding-accuracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> no no no notorious <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì¥ ìƒì„±2\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> no no\", max_len=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-connecticut",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-title",
   "metadata": {},
   "source": [
    "## 5. ê³ ì°°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-ladder",
   "metadata": {},
   "source": [
    "- **validation loss**   \n",
    "1. embedding_sizeë¥¼ 2ë°°ë¡œ ëŠ˜ë ¸ë”ë‹ˆ validation lossê°€ ì¡°ê¸ˆ ë‚®ì•„ì§ì„ í™•ì¸í•˜ì˜€ë‹¤.\n",
    "2. hidden_sizeë¥¼ ì•½ 1.5ë°°ë¡œ ëŠ˜ë ¸ë”ë‹ˆ validation lossê°€ ì¢€ ë” ë‚®ì•„ì§ì„ í™•ì¸í•˜ì˜€ë‹¤.\n",
    "3. ëª©í‘œì¹˜ì™€ ê·¼ì‚¬í•˜ê²Œ lossê°’ì´ ë‚´ë ¤ì™”ë‹¤. íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ë¥¼ ì¢€ ë” ëŠ˜ë ¤ ëª©í‘œì¹˜ ì´í•˜ë¡œ lossë¥¼ ë‚´ë¦¬ë ¤ë‹¤ê°€ í•œì‹œê°„ ì´ìƒì˜ í›ˆë ¨ì‹œê°„ì„ ë” ê¸°ë‹¤ë¦¬ëŠ”ê²Œ ì˜ë¯¸ê°€ ì—†ì„ ë“¯ ì‹¶ì–´ ë©ˆì¶”ì—ˆë‹¤. ëŒ€ì‹  layerì— ëŒ€í•œ ê³µë¶€ë¥¼ í†µí•´ ë‹¤ìŒë²ˆì—” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ lossê°’ì„ ì¤„ì—¬ë³¼ ì˜ˆì •ì´ë‹¤.\n",
    "\n",
    "\n",
    "- **ë¬¸ì¥ ìƒì„±**   \n",
    "10ë‹¨ì–´ ì´ìƒì˜ ë¬¸ì¥ì„ ì˜ˆìƒí–ˆìœ¼ë‚˜ ì§§ì€ ë¬¸ì¥ë§Œì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. ìƒê°í•´ë³´ë‹ˆ ë¬¸ì¥ë§ˆë‹¤ íŒ¨ë”©ì²˜ë¦¬ë˜ëŠ” ë¶€ë¶„ì´ ë§ë˜ ê²ƒì´ ê¸°ì–µë‚¬ë‹¤. ë…¸ë˜ ê°€ì‚¬ ìì²´ê°€ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆì–´ ê·¸ë ‡ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.\n",
    "<br>   \n",
    "\n",
    "- **í›ˆë ¨ ì‹œê°„**   \n",
    "ì²˜ìŒì—” ëŠë¼ì§€ ëª»í–ˆëŠ”ë° ì˜¤ëŠ˜ ë…¸ë“œì—ì„œ ëª¨ë¸ í›ˆë ¨ì‹œê°„ì´ ê½¤ë‚˜ ê±¸ë¦¬ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤. ì•ìœ¼ë¡œëŠ” í›¨ì”¬ í° ë°ì´í„°ë¥¼ ê¹Šì€ ì¸µìœ¼ë¡œ í›ˆë ¨ì‹œì¼œì•¼ í• í…ë° ê±±ì •ì´ë‹¤. colabì—ì„œ ì¢€ë” ë¹ ë¥¸ í›ˆë ¨ì´ ê°€ëŠ¥í•œì§€ í™•ì¸í•œ í›„ ê¸´ í›ˆë ¨ì‹œê°„ì´ ì˜ˆìƒë˜ëŠ” ë…¸ë“œì˜ ê²½ìš° ì´ê³³ì—ì„œ í›ˆë ¨ì‹œì¼œë³¼ ì˜ˆì •ì´ë‹¤. í›ˆë ¨ ì‹œê°„ ì „ì— ìµœëŒ€í•œ ë¹ ë¥´ê²Œ ì½”ë“œë¥¼ ì™„ì„±í•˜ê³  íœ´ì‹ì„ ì·¨í•˜ëŠ” ê²ƒë„ ì¢‹ì„ ë“¯í•˜ë‹¤.\n",
    "<br>   \n",
    "\n",
    "- **ì¶”ê°€ì ìœ¼ë¡œ ê³µë¶€í•  ë¶€ë¶„**   \n",
    "validation lossë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ layerë¥¼ ìŒ“ëŠ” ë°©ì•ˆì´ ìˆìŒì„ ì•Œê³  ìˆìœ¼ë‚˜ ê° layerì— ëŒ€í•œ ì´í•´ê°€ ì•„ì§ ë¶€ì¡±í•œ ìƒí™©ì´ë‹¤. ì•ìœ¼ë¡œ ê³„ì† ëª¨ë¸ í›ˆë ¨ì´ ì§„í–‰ë ê²ƒìœ¼ë¡œ ìƒê°ë˜ëŠ”ë§Œí¼ layerì—ëŒ€í•œ ê³µë¶€ë¥¼ ì§„í–‰í•  ì˜ˆì •ì´ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
