{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-mozambique",
   "metadata": {},
   "source": [
    "# 🎼 인공지능 작사가 만들기\n",
    "\n",
    "#### \"단어를 넣으면 가사를(한 줄) 써주는 프로그램 제작\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-queue",
   "metadata": {},
   "source": [
    "### 😮 목차\n",
    "\n",
    "**1. 데이터 준비**   \n",
    "**2. 데이터 전처리**     \n",
    "**3. 모델 설계, 훈련, 평가**   \n",
    "**4. 가사 생성하기**   \n",
    "**5. 고찰**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-particular",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-privacy",
   "metadata": {},
   "source": [
    "**모듈**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naval-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-improvement",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-present",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatty-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\", 'I grew strong', \"I learned how to get along And so you're back\", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock', 'I would have made you leave your key', 'If I had known for just one second', \"You'd be back to bother me Well now go,\", 'Walk out the door', 'Just turn around', \"Now, you're not welcome anymore Weren't you the one\", 'Who tried to break me with desire?', \"Did you think I'd crumble?\", \"Did you think I'd lay down and die? Oh not I,\", 'I will survive', 'Yeah', \"As Long as I know how to love, I know I'll be alive\", \"I've got all my life to live\", \"I've got all my love to give\", 'I will survive, I will survive', 'Yeah, yeah', 'It took all the strength I had', \"Just not to fall apart I'm trying hard to mend the pieces\", 'Of my broken heart', 'And I spent oh so many nights']\n"
     ]
    }
   ],
   "source": [
    "text_file_path = os.getenv('HOME')+'/aiffel/aiffel_exploration/lyricist/data/lyrics/*'\n",
    "text_list = glob.glob(text_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "for text_file in text_list:\n",
    "    with open(text_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-policy",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-natural",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-catholic",
   "metadata": {},
   "source": [
    "### 2.1 문장 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparable-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 넣기\n",
    "    sentence = re.sub(r\"[' ']+\", ' ', sentence) # 여러 공백은 하나로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", ' ', sentence) # 해당문자 이외의 문자를 공백으로 바꾸기\n",
    "    sentence = sentence.strip() # 양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장의 시작과 끝에 라벨 붙이기\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decimal-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> i would have made you leave your key <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 문장 모으기\n",
    "corpus = []\n",
    "for sentence in raw_corpus: # 한 문장씩 꺼내기\n",
    "    if len(sentence) == 0 or len(sentence) == 1: # 문장의 길이가 0, 1이면 저장안함\n",
    "        continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence) # 문장 정제하기\n",
    "    if len(preprocessed_sentence.split()) > 15: # 정제된 문장이 15단어 이상이면 저장안함\n",
    "        continue     \n",
    "    corpus.append(preprocessed_sentence) # 리스트 생성\n",
    "\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-karaoke",
   "metadata": {},
   "source": [
    "### 2.2 문장 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boring-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ', oov_token=\"<unk>\") # 12000단어를 기억, 이미 문장 정제해서 filter 필요 없음, 12000단어 이외의 단어는 \"<unk>\"로 취급\n",
    "    tokenizer.fit_on_texts(corpus) # 단어사전 생성\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 데이터를 벡터화\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # 입력데이터 시퀸스 길이를 일정하게 맞춤. 시퀀스가 짧으면 문장 뒤에 패딩 붙음. padding='pre'는 앞에 붙음\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus) # 문장, 단어사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distinguished-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156192, 15)\n",
      "[[   2   70  247    4   53  708    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    4   53 6269    3    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    4 1066  525    4  104   80  192  257    7    3    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(tensor.shape) # 15개의 단어로 구성된 156192개의 문장\n",
    "print(tensor[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beneficial-tradition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n"
     ]
    }
   ],
   "source": [
    "# 단어사전 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-puppy",
   "metadata": {},
   "source": [
    "### 2.3 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "through-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  70 247   4  53 708   3   0   0   0   0   0   0   0]\n",
      "[ 70 247   4  53 708   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 소스문장, 타겟문장 생성하기\n",
    "src_input = tensor[:, :-1] # 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <start>를 잘라내서 타겟 문장 생성\n",
    "\n",
    "print(src_input[0]) # 맨 뒤 인덱스 제거 상태\n",
    "print(tgt_input[0]) # 맨 앞 인덱스 제거 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifty-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124953, 14)\n",
      "Target Train: (124953, 14)\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터, 평가데이터 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=9)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "federal-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터 설정\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-cisco",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-center",
   "metadata": {},
   "source": [
    "## 3. 모델 설계, 훈련, 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-demographic",
   "metadata": {},
   "source": [
    "### 3.1 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beginning-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # 단어사전의 인덱스를 해당 인덱스의 워드 벡터로 바꿈 \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True) \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "vocab_size = tokenizer.num_words + 1 # 12000 + pad  \n",
    "embedding_size = 256\n",
    "hidden_size = 1500\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fallen-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 3.49633367e-04, -4.79755865e-04,  2.33902945e-04, ...,\n",
       "         -1.96660967e-05, -4.43540339e-04,  4.02360543e-04],\n",
       "        [ 3.23095592e-04, -5.30259917e-04,  1.94919368e-04, ...,\n",
       "         -2.16977660e-05, -4.78130125e-04,  1.00855366e-03],\n",
       "        ...,\n",
       "        [ 2.19061249e-03, -7.25672347e-04,  4.15295595e-04, ...,\n",
       "          1.65148627e-03, -4.48797422e-04,  2.24689837e-03],\n",
       "        [ 2.58490234e-03, -2.63625989e-04,  4.60571813e-04, ...,\n",
       "          1.91962195e-03, -2.33269966e-04,  2.28238688e-03],\n",
       "        [ 2.93728127e-03,  2.19953203e-04,  4.89130209e-04, ...,\n",
       "          2.11978983e-03, -1.57870127e-05,  2.32613203e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 5.19168061e-05, -2.92468263e-04,  6.83102626e-05, ...,\n",
       "          9.85752704e-05, -4.62379598e-04, -5.52645943e-05],\n",
       "        [ 1.52741966e-04, -4.44839912e-04,  1.01745027e-04, ...,\n",
       "         -4.09482818e-05, -5.13952400e-04, -1.26020997e-04],\n",
       "        ...,\n",
       "        [ 1.83100789e-03,  4.78931353e-04,  4.56420297e-04, ...,\n",
       "          2.06716172e-03,  4.67486505e-04,  1.20177958e-03],\n",
       "        [ 2.27331859e-03,  9.02572239e-04,  5.20350120e-04, ...,\n",
       "          2.24718312e-03,  6.39829144e-04,  1.44978438e-03],\n",
       "        [ 2.67182570e-03,  1.32721767e-03,  5.70759934e-04, ...,\n",
       "          2.37469561e-03,  7.97321089e-04,  1.67556701e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [-4.18869386e-05, -3.84855201e-04, -1.78509028e-04, ...,\n",
       "          1.17171490e-04, -2.49970413e-04,  3.71594360e-04],\n",
       "        [-2.87309173e-04, -3.12132121e-04, -3.08558054e-04, ...,\n",
       "         -9.86980085e-05, -2.32570455e-04,  6.26410241e-04],\n",
       "        ...,\n",
       "        [ 1.04327302e-03,  4.50637337e-04, -2.23918294e-04, ...,\n",
       "          1.04025577e-03,  7.13730638e-04,  1.30405044e-03],\n",
       "        [ 1.49795495e-03,  6.30922092e-04, -1.52855253e-04, ...,\n",
       "          1.39481912e-03,  8.38164182e-04,  1.47304509e-03],\n",
       "        [ 1.92514271e-03,  8.77473154e-04, -7.61426563e-05, ...,\n",
       "          1.69858744e-03,  9.50766902e-04,  1.64174242e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [-1.08631139e-04, -1.26508749e-04, -2.57421983e-04, ...,\n",
       "         -4.66796337e-05, -4.96760709e-04, -4.26693296e-05],\n",
       "        [-1.36202361e-04, -1.35887152e-04, -3.32299416e-04, ...,\n",
       "         -3.75149975e-05, -5.71071054e-04, -2.19226044e-04],\n",
       "        ...,\n",
       "        [ 2.09059892e-03,  5.62418893e-04,  4.11125366e-04, ...,\n",
       "          2.13874341e-03,  3.77763790e-04,  6.70402602e-04],\n",
       "        [ 2.46163574e-03,  8.89785704e-04,  4.53985442e-04, ...,\n",
       "          2.34731100e-03,  6.14915625e-04,  9.57512937e-04],\n",
       "        [ 2.80239363e-03,  1.24323950e-03,  4.90898616e-04, ...,\n",
       "          2.49508861e-03,  8.32190912e-04,  1.22424553e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 1.80454481e-05, -4.57068672e-04,  3.52210336e-05, ...,\n",
       "          6.03899862e-05, -3.34966142e-04, -9.82261481e-05],\n",
       "        [-1.86154226e-04, -7.27543142e-04, -7.16369395e-05, ...,\n",
       "          2.91030126e-04, -2.31985061e-04, -1.38864663e-04],\n",
       "        ...,\n",
       "        [ 2.23370595e-03,  5.23134950e-04,  5.78967680e-04, ...,\n",
       "          1.75671827e-03,  7.22748809e-04,  1.58406631e-03],\n",
       "        [ 2.65683001e-03,  9.53179668e-04,  6.34717464e-04, ...,\n",
       "          1.95636274e-03,  8.40177294e-04,  1.78415619e-03],\n",
       "        [ 3.02647869e-03,  1.36979774e-03,  6.65608502e-04, ...,\n",
       "          2.10457179e-03,  9.52058996e-04,  1.96472183e-03]],\n",
       "\n",
       "       [[ 7.28329978e-05, -2.68868316e-04, -2.59418066e-05, ...,\n",
       "          6.88638247e-05, -1.90609499e-04, -2.98106715e-06],\n",
       "        [ 2.41917965e-04, -5.17658307e-04, -4.72484026e-05, ...,\n",
       "          2.58169166e-04, -1.17056516e-04,  5.91853423e-05],\n",
       "        [ 2.06966404e-04, -6.45298511e-04,  1.02968763e-04, ...,\n",
       "          3.06783826e-04, -9.98731630e-05,  2.58485903e-04],\n",
       "        ...,\n",
       "        [ 2.17729318e-03,  7.43298617e-04,  3.02359113e-04, ...,\n",
       "          2.00338592e-03,  5.24481933e-04,  2.20783567e-03],\n",
       "        [ 2.56654597e-03,  1.15957833e-03,  3.29909293e-04, ...,\n",
       "          2.18390441e-03,  6.58618344e-04,  2.31957505e-03],\n",
       "        [ 2.91374419e-03,  1.56851392e-03,  3.45860433e-04, ...,\n",
       "          2.31188349e-03,  7.86948600e-04,  2.41753273e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 본격적인 학습 전 모델에 데이터 일부 태워보기\n",
    "for src_sample, tgt_sample in dataset.take(1): # take(1) 1개의 배치. 256개의 문장 데이터 가져옴\n",
    "    break\n",
    "    \n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecological-oxygen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  10542000  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  18006000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  18013501  \n",
      "=================================================================\n",
      "Total params: 49,633,757\n",
      "Trainable params: 49,633,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # 출력 형태가 비정형. 모델이 입력 시퀀스 길이를 모르기 때문에 임의의 값을 넣어주면 설정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-density",
   "metadata": {},
   "source": [
    "### 3.2 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vital-ferry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "488/488 [==============================] - 271s 550ms/step - loss: 4.0288\n",
      "Epoch 2/10\n",
      "488/488 [==============================] - 271s 554ms/step - loss: 3.0689\n",
      "Epoch 3/10\n",
      "488/488 [==============================] - 272s 558ms/step - loss: 2.8699\n",
      "Epoch 4/10\n",
      "488/488 [==============================] - 273s 559ms/step - loss: 2.7071\n",
      "Epoch 5/10\n",
      "488/488 [==============================] - 273s 560ms/step - loss: 2.5744\n",
      "Epoch 6/10\n",
      "488/488 [==============================] - 274s 560ms/step - loss: 2.4598\n",
      "Epoch 7/10\n",
      "488/488 [==============================] - 272s 558ms/step - loss: 2.3521\n",
      "Epoch 8/10\n",
      "488/488 [==============================] - 273s 558ms/step - loss: 2.2422\n",
      "Epoch 9/10\n",
      "488/488 [==============================] - 273s 558ms/step - loss: 2.1465\n",
      "Epoch 10/10\n",
      "488/488 [==============================] - 272s 558ms/step - loss: 2.0535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ed03ae910>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-constraint",
   "metadata": {},
   "source": [
    "### 3.3 모델  평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conditional-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977/977 [==============================] - 47s 47ms/step - loss: 2.4457\n",
      "val_loss: 2.4456546306610107\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val, dec_val)\n",
    "print(\"val_loss:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-majority",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-mechanism",
   "metadata": {},
   "source": [
    "## 4. 가사 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "juvenile-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성기\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=17):\n",
    "    # 테스트를 위해 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True: # 텍스트를 실제로 생성해야 하는 시점에서 소스문장, 타겟문장이 없다.\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 예측 단어 생성\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # 생성 단어 이어붙이기\n",
    "        if predict_word.numpy()[0] == end_token: # 마지막이 <end>면 종료\n",
    "            break\n",
    "        if test_tensor.shape[1] > max_len: # <start>, <end> 제외한 단어 갯수가 15를 넘기면 종료 \n",
    "            break\n",
    "            \n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy(): # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "designed-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the one <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 생성1\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "binding-accuracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> no no no notorious <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 생성2\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> no no\", max_len=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-connecticut",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-title",
   "metadata": {},
   "source": [
    "## 5. 고찰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-ladder",
   "metadata": {},
   "source": [
    "- **validation loss**   \n",
    "1. embedding_size를 2배로 늘렸더니 validation loss가 조금 낮아짐을 확인하였다.\n",
    "2. hidden_size를 약 1.5배로 늘렸더니 validation loss가 좀 더 낮아짐을 확인하였다.\n",
    "3. 목표치와 근사하게 loss값이 내려왔다. 파라미터의 크기를 좀 더 늘려 목표치 이하로 loss를 내리려다가 한시간 이상의 훈련시간을 더 기다리는게 의미가 없을 듯 싶어 멈추었다. 대신 layer에 대한 공부를 통해 다음번엔 다른 방식으로 loss값을 줄여볼 예정이다.\n",
    "\n",
    "\n",
    "- **문장 생성**   \n",
    "10단어 이상의 문장을 예상했으나 짧은 문장만을 생성하는 것을 확인하였다. 생각해보니 문장마다 패딩처리되는 부분이 많던 것이 기억났다. 노래 가사 자체가 짧은 문장으로 이루어져 있어 그렇다는 생각이 들었다.\n",
    "<br>   \n",
    "\n",
    "- **훈련 시간**   \n",
    "처음엔 느끼지 못했는데 오늘 노드에서 모델 훈련시간이 꽤나 걸리는 것을 깨달았다. 앞으로는 훨씬 큰 데이터를 깊은 층으로 훈련시켜야 할텐데 걱정이다. colab에서 좀더 빠른 훈련이 가능한지 확인한 후 긴 훈련시간이 예상되는 노드의 경우 이곳에서 훈련시켜볼 예정이다. 훈련 시간 전에 최대한 빠르게 코드를 완성하고 휴식을 취하는 것도 좋을 듯하다.\n",
    "<br>   \n",
    "\n",
    "- **추가적으로 공부할 부분**   \n",
    "validation loss를 낮추기 위해 추가적인 layer를 쌓는 방안이 있음을 알고 있으나 각 layer에 대한 이해가 아직 부족한 상황이다. 앞으로 계속 모델 훈련이 진행될것으로 생각되는만큼 layer에대한 공부를 진행할 예정이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
